{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3d1282f"
      },
      "source": [
        "# **Stock Price Prediction using LSTM**\n",
        "\n",
        "\n",
        "### Problem Statement\n",
        "Financial markets are volatile and complex, making accurate stock price prediction challenging.  \n",
        "This project aims to predict future stock closing prices using historical data and deep learning techniques.\n",
        "\n",
        "### Objective\n",
        "- Predict future stock prices using historical trends  \n",
        "- Capture temporal dependencies using LSTM  \n",
        "- Evaluate model performance using appropriate metrics  \n",
        "\n",
        "### Real-World Relevance & Motivation\n",
        "- Supports data-driven investment decisions  \n",
        "- Helps in trend identification and risk analysis  \n",
        "- Demonstrates real-world application of AI in finance\n",
        "\n",
        "Dataset source: Public dataset collected from kaggle as on 27 Dec 2025\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK-NGkPGKvkf"
      },
      "source": [
        "# **Exploratory Data Analysis and Data Pre Processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWW82dTEEI2h"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "df = pd.read_csv(\"NIFTY_50_COMPANIES.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loaded the dataset using pandas and exploring dataset"
      ],
      "metadata": {
        "id": "xgEADwFl9q5j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRlZeNL_CdR9"
      },
      "outputs": [],
      "source": [
        "df = df[df['Ticker'] == 'RELIANCE.NS']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only Reliance stock is selected in the ticker as the sock prediction model is for a single stock"
      ],
      "metadata": {
        "id": "XQps0VEt7xcM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqmzGtOTINCS"
      },
      "outputs": [],
      "source": [
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df = df.sort_values('Date')\n",
        "df.set_index('Date', inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converted the date column to datetime format"
      ],
      "metadata": {
        "id": "H3N6_lhP9DRH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HE5jNnzIG5SB"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpAD_S6XHnrg"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaaqHDpdHs-D"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No missing values for the relevant data used in model"
      ],
      "metadata": {
        "id": "p_NcHFf6-39F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmbD-dTnIdsz"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(df['Close'])\n",
        "plt.title(\"Closing Price Trend\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Closing price long term trend exhibits sequential pattern which is perfect for forecasting time-series data"
      ],
      "metadata": {
        "id": "nJW3_kgp_zDH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ztc7_nJIma2"
      },
      "outputs": [],
      "source": [
        "df['Daily_Return'] = df['Close'].pct_change()\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(df['Daily_Return'])\n",
        "plt.title(\"Daily Returns\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Daily returns over the years are almost linear. The spikes are due to extreme events like stock split. We ignore these data and proceed ahead."
      ],
      "metadata": {
        "id": "VvyxwE4XATw3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNMRRy9aJE6a"
      },
      "outputs": [],
      "source": [
        "df['MA20'] = df['Close'].rolling(20).mean()\n",
        "df['MA50'] = df['Close'].rolling(50).mean()\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(df['Close'], label='Close')\n",
        "plt.plot(df['MA20'], label='20-Day MA')\n",
        "plt.plot(df['MA50'], label='50-Day MA')\n",
        "plt.legend()\n",
        "plt.title(\"Moving Average Analysis\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The moving averages smooth price fluctuations which helps in analysing short and medium term trends. This confirms the bullish market in the long run"
      ],
      "metadata": {
        "id": "RVbl-TCCBN4X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXYmRW57JNOv"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(\n",
        "    df[['Open','High','Low','Close','Volume']].corr(),\n",
        "    annot=True,\n",
        "    cmap='coolwarm'\n",
        ")\n",
        "plt.title(\"Feature Correlation Heatmap\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stock price related data are highly correlated. Focus on closing price to avoid redundancy."
      ],
      "metadata": {
        "id": "HZ-OysC0BuSW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mW6CYBn6JXuk"
      },
      "outputs": [],
      "source": [
        "sns.boxplot(data=df[['Open','High','Low','Close']])\n",
        "plt.title(\"Price Distribution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The price distribution shows outliers in the data over time but cannot ignore them as it is crucial for prediction."
      ],
      "metadata": {
        "id": "JA9lgzR6DihZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xauzwsmK60s"
      },
      "source": [
        "# **Feature Engineering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dVP05ipLL4u"
      },
      "outputs": [],
      "source": [
        "data = df[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "data = data.values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used only relavant data for prediction"
      ],
      "metadata": {
        "id": "jU3feKobEhei"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MD095RF3Tmbs"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalised data using Min-Max scaling"
      ],
      "metadata": {
        "id": "HWSkTreLIjKe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBI23DPiVxIl"
      },
      "outputs": [],
      "source": [
        "window_size=60\n",
        "X, y = [], []\n",
        "\n",
        "for i in range(window_size, len(scaled_data)):\n",
        "    X.append(scaled_data[i - window_size : i])\n",
        "    y.append(scaled_data[i, 3])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nNmQfSIUnqLY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2psrLUtSV-js"
      },
      "outputs": [],
      "source": [
        "split = int(0.8 * len(X))\n",
        "\n",
        "X_train = X[:split]\n",
        "X_test  = X[split:]\n",
        "\n",
        "y_train = y[:split]\n",
        "y_test  = y[split:]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VC0cW83KoRyq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3edGmhJlaqC"
      },
      "outputs": [],
      "source": [
        "WINDOW_SIZES = [60]\n",
        "LSTM_UNITS = [32, 50]\n",
        "BATCH_SIZES = [32]\n",
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJ8EtICnlnm8"
      },
      "outputs": [],
      "source": [
        "def build_lstm_model(units, input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=input_shape))\n",
        "    model.add(LSTM(units, return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(units))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "def create_sequences(data, window_size):\n",
        "    X, y = [], []\n",
        "    for i in range(window_size, len(data)):\n",
        "        X.append(data[i-window_size:i])\n",
        "        y.append(data[i, 3])\n",
        "    return np.array(X), np.array(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used 60 days as data framework sequences for more reliable long term pattern learning"
      ],
      "metadata": {
        "id": "qxBxJOv2oeNK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uma08LaG5Mn6"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
        "results = []\n",
        "\n",
        "for window in WINDOW_SIZES:\n",
        "    X, y = create_sequences(scaled_data, window)\n",
        "\n",
        "    split = int(0.8 * len(X))\n",
        "    X_train, X_test = X[:split], X[split:]\n",
        "    y_train, y_test = y[:split], y[split:]\n",
        "\n",
        "    for units in LSTM_UNITS:\n",
        "        for batch in BATCH_SIZES:\n",
        "\n",
        "            model = build_lstm_model(\n",
        "                units,\n",
        "                input_shape=(X_train.shape[1], X_train.shape[2])\n",
        "            )\n",
        "\n",
        "            history = model.fit(\n",
        "                X_train, y_train,\n",
        "                epochs=20,\n",
        "                batch_size=batch,\n",
        "                validation_split=0.1,\n",
        "                verbose=0\n",
        "            )\n",
        "\n",
        "            val_loss = min(history.history['val_loss'])\n",
        "\n",
        "            results.append({\n",
        "                'window': window,\n",
        "                'units': units,\n",
        "                'batch': batch,\n",
        "                'val_loss': val_loss\n",
        "            })"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data is split chronologicaly for accurate training and testing. Trained multiple LSTM configurations and selected the best model based on validation loss"
      ],
      "metadata": {
        "id": "m3s7u2L6SpHc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQ3jyQa1WCnn"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))\n",
        "\n",
        "model.add(LSTM(50, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(50))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used a stacked LSTM with dropout to capture long-term dependencies while preventing overfitting."
      ],
      "metadata": {
        "id": "yXzJ6CcJcpXd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Gj0az3MXekM"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mean_squared_error'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adam is used for fast, stable convergence with adaptive learning rates, and MSE is used because it strongly penalizes large errors, making it suitable for regression-based price prediction."
      ],
      "metadata": {
        "id": "3MRFet-XdCl1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-AACMBhXfqv"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " The model was trained for 20 epochs with a batch size of 32 and a 10% validation split.Training loss consistently decreases. Validation loss remains low and stable. However minor fluctuations in validation loss are expected due to market volatility and noise."
      ],
      "metadata": {
        "id": "_xDMvX_zdMqi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGqHxUt9Y998"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss curves show stable learning without overfitting, indicating good model generalization"
      ],
      "metadata": {
        "id": "0xSHVfwedm51"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dieqcRAnZmM0"
      },
      "outputs": [],
      "source": [
        "predicted = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used the trained model to predict stock prices on unseen test data"
      ],
      "metadata": {
        "id": "VpVhUQfpd3lY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArfMw29wZtVe"
      },
      "outputs": [],
      "source": [
        "dummy_pred = np.zeros((len(predicted), 5))\n",
        "dummy_pred[:, 3] = predicted[:, 0]\n",
        "\n",
        "predicted_prices = scaler.inverse_transform(dummy_pred)[:, 3]\n",
        "\n",
        "dummy_actual = np.zeros((len(y_test), 5))\n",
        "dummy_actual[:, 3] = y_test\n",
        "\n",
        "actual_prices = scaler.inverse_transform(dummy_actual)[:, 3]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inverse scaling the predicted closing prices to its actual values"
      ],
      "metadata": {
        "id": "FJV4qJ_yeJ9e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGP0seEKZ19N"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(actual_prices, label=\"Actual Price\")\n",
        "plt.plot(predicted_prices, label=\"Predicted Price\")\n",
        "plt.legend()\n",
        "plt.title(\"LSTM Stock Price Prediction\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a close match between predicted price and the actual price"
      ],
      "metadata": {
        "id": "dhUSOwk_eeWt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHM-x4VWZ98R"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(actual_prices, predicted_prices))\n",
        "mae = mean_absolute_error(actual_prices, predicted_prices)\n",
        "\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"MAE:\", mae)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lower values of MAE and RMSE indicate better predictive performance, and these results suggest that the LSTM model performs pretty well"
      ],
      "metadata": {
        "id": "LAl4xL_Jexr_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vszJzYyF1yz1"
      },
      "outputs": [],
      "source": [
        "n_days = 1\n",
        "\n",
        "last_window = scaled_data[-window_size:]\n",
        "\n",
        "last_window = last_window.reshape(1, window_size, scaled_data.shape[1])\n",
        "\n",
        "\n",
        "next_day_scaled = model.predict(last_window)\n",
        "\n",
        "\n",
        "dummy = np.zeros((1, scaled_data.shape[1]))\n",
        "dummy[0, 3] = next_day_scaled[0, 0]\n",
        "\n",
        "next_day_price = scaler.inverse_transform(dummy)[0, 3]\n",
        "\n",
        "print(\"Predicted next day closing price:\", next_day_price)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the last 60 days of data, the LSTM predicts the next day’s closing price after inverse scaling."
      ],
      "metadata": {
        "id": "KyejDCw1f2d-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ON-l0lgK2hJ9"
      },
      "outputs": [],
      "source": [
        "def predict_future_days(model, scaled_data, window_size, n_days):\n",
        "    future_predictions = []\n",
        "\n",
        "    current_window = scaled_data[-window_size:].copy()\n",
        "\n",
        "    for _ in range(n_days):\n",
        "        X = current_window.reshape(1, window_size, scaled_data.shape[1])\n",
        "        next_scaled = model.predict(X)[0, 0]\n",
        "\n",
        "        future_predictions.append(next_scaled)\n",
        "\n",
        "        next_row = current_window[-1].copy()\n",
        "        next_row[3] = next_scaled\n",
        "        current_window = np.vstack([current_window[1:], next_row])\n",
        "\n",
        "    return future_predictions\n",
        "\n",
        "n_days = 10\n",
        "future_scaled = predict_future_days(model, scaled_data, window_size, n_days)\n",
        "\n",
        "\n",
        "future_prices = []\n",
        "for val in future_scaled:\n",
        "    dummy = np.zeros((1, scaled_data.shape[1]))\n",
        "    dummy[0, 3] = val\n",
        "    future_prices.append(scaler.inverse_transform(dummy)[0, 3])\n",
        "\n",
        "print(\"Future predicted prices:\")\n",
        "for i, price in enumerate(future_prices, 1):\n",
        "    print(f\"Day {i}: {price}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used recursive forecasting where each predicted day is fed back as input to predict multiple future days"
      ],
      "metadata": {
        "id": "daCBiUeYgWpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation and Analysis**"
      ],
      "metadata": {
        "id": "7Olo6oDngZjs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoS8Yg-f3eWF"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(actual_prices, predicted_prices))\n",
        "mae = mean_absolute_error(actual_prices, predicted_prices)\n",
        "mape = np.mean(np.abs((actual_prices - predicted_prices) / actual_prices)) * 100\n",
        "r2 = r2_score(actual_prices, predicted_prices)\n",
        "\n",
        "direction_actual = np.sign(np.diff(actual_prices))\n",
        "direction_pred = np.sign(np.diff(predicted_prices))\n",
        "directional_accuracy = np.mean(direction_actual == direction_pred) * 100\n",
        "\n",
        "print(f\"RMSE: {rmse}\")\n",
        "print(f\"MAE: {mae}\")\n",
        "print(f\"MAPE: {mape}%\")\n",
        "print(f\"R² Score: {r2}\")\n",
        "print(f\"Directional Accuracy: {directional_accuracy}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MAE is robust to outliers, indicating consistent prediction accuracy across most periods. Given that the stock price ranges between ₹400 and ₹1600, this error is relatively small.\n",
        "\n",
        "RMSE being higher than MAE indicates the presence of occasional larger errors, typically during high-volatility periods. This is expected in financial time-series due to sudden market movements and external shocks.\n",
        "\n",
        "In financial forecasting, a MAPE below 5% is generally considered strong performance.\n",
        "\n",
        "An R² of 0.98 indicates a very strong fit and confirms that the LSTM has effectively learned long-term price patterns.\n",
        "\n",
        "A directional accuracy close to 50% suggests performance similar to random guessing for short-term direction. Reasons for Low Directional Accuracy might be that the stock prices are influenced by news, sentiment, macroeconomic events, and sudden shocks."
      ],
      "metadata": {
        "id": "26u5K2ZtiAh7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **AI and Biasness**"
      ],
      "metadata": {
        "id": "IA4LN_3HkBXt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The absence of external factors such as macroeconomic indicators, company fundamentals, or news sentiment introduces representation bias, as the model captures only technical price movements. Evaluation metrics focused on error reduction may also mask directional or risk-related biases, giving an overly optimistic view of model performance."
      ],
      "metadata": {
        "id": "XiGEFRtVkCLP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "6R7roOcqkLoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project successfully demonstrates the application of an LSTM-based deep learning model for stock price prediction using historical market data. The model was able to capture temporal patterns and trends in stock prices, producing reasonably accurate forecasts based on past price movements. Data preprocessing, feature scaling, and sequence modeling played a critical role in improving model performance.\n",
        "\n",
        "Overall, the project highlights the potential of LSTM networks as effective tools for time-series forecasting in financial markets. While the model provides useful predictive insights, its outputs are best suited for decision support rather than standalone investment decisions, especially in dynamic market conditions. The project also establishes a strong foundation for future enhancements through additional data sources and advanced evaluation techniques."
      ],
      "metadata": {
        "id": "MMA0mjmekSVD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Future Scope**"
      ],
      "metadata": {
        "id": "C78awL4nk9aa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project can be further enhanced by incorporating additional data sources such as macroeconomic indicators, company fundamentals, and news sentiment to improve predictive accuracy. Advanced deep learning architectures like GRU, CNN-LSTM hybrids, or attention-based models can be explored to better capture complex market dynamics.\n",
        "\n",
        "Future work may also include multi-stock or portfolio-level prediction, improved evaluation using directional and risk-based metrics, and real-time model retraining to adapt to changing market conditions. Integrating the model into a decision-support system for trading or risk analysis would increase its practical applicability."
      ],
      "metadata": {
        "id": "Sd7YZmrTk-HH"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}